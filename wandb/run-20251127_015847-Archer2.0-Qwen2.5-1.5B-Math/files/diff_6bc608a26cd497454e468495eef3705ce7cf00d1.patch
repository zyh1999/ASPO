diff --git a/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh b/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh
index dd300af..1f3af06 100644
--- a/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh
+++ b/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh
@@ -20,12 +20,12 @@ clip_ratio_low=0.2
 clip_ratio_high=0.2
 loss_agg_mode=token-mean
 
-max_prompt_length=$((1024 * 2))
-max_response_length=$((1024 * 4))
+max_prompt_length=$((1024 * 1))
+max_response_length=$((1024 * 1))
 enable_overlong_buffer=True
 overlong_buffer_len=16
 overlong_penalty_factor=1.0
-v_max_response_length=$((1024 * 4))
+v_max_response_length=$((1024 * 1))
 
 train_prompt_bsz=64
 gen_prompt_bsz=$((train_prompt_bsz * 1))
@@ -55,7 +55,8 @@ v_top_k=-1
 sp_size=1
 gen_tp=1
 use_dynamic_bsz=False
-micro_batch_size_per_gpu=1
+# 2x80G A100: 提高每卡 micro batch 利用显存（如 OOM 再降回 4）
+micro_batch_size_per_gpu=8
 actor_ppo_max_token_len=$((max_prompt_length + v_max_response_length))
 infer_ppo_max_token_len=$((max_prompt_length + v_max_response_length))
 offload=False
@@ -165,8 +166,10 @@ python -m dapo.main_dapo \
     actor_rollout_ref.actor.grad_clip=1.0 \
     actor_rollout_ref.actor.loss_agg_mode=${loss_agg_mode} \
     actor_rollout_ref.actor.ulysses_sequence_parallel_size=${sp_size} \
-    actor_rollout_ref.rollout.gpu_memory_utilization=0.6\
+    actor_rollout_ref.rollout.gpu_memory_utilization=0.7\
     actor_rollout_ref.rollout.tensor_model_parallel_size=${gen_tp} \
+    actor_rollout_ref.rollout.enforce_eager=False \
+    actor_rollout_ref.rollout.free_cache_engine=False \
     actor_rollout_ref.rollout.enable_chunked_prefill=True \
     actor_rollout_ref.rollout.max_num_batched_tokens=$((max_prompt_length + v_max_response_length)) \
     actor_rollout_ref.rollout.max_model_len=$((max_prompt_length + v_max_response_length)) \
@@ -193,8 +196,8 @@ python -m dapo.main_dapo \
     trainer.nnodes="${nnodes}" \
     trainer.balance_batch=False \
     trainer.val_before_train=False \
-    trainer.test_freq=-1 \
-    trainer.save_freq=40 \
+    trainer.test_freq=20 \
+    trainer.save_freq=20 \
     trainer.total_epochs=2 \
     trainer.default_local_dir="${CKPTS_DIR}" \
     trainer.resume_mode=auto \
