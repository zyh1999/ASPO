#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@Time    :   2025/06/21 18:55:18
@Author  :   wangjiakang
@File    :   main_generation.py
'''


import csv
import ray
import numpy as np
import hydra
import os
import wandb
from tabulate import tabulate

os.environ["NCCL_DEBUG"] = "WARN"
os.environ["TOKENIZERS_PARALLELISM"] = "true"
# os.environ['TORCH_COMPILE_DISABLE'] = '1'

from pprint import pprint

import pandas as pd
from omegaconf import OmegaConf
from tqdm import tqdm

from verl import DataProto
from verl.protocol import pad_dataproto_to_divisor, unpad_dataproto
from verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup
from verl.utils import hf_tokenizer
from verl.utils.fs import copy_to_local
from verl.utils.hdfs_io import makedirs
from verl.utils.model import compute_position_id_with_mask
from verl.workers.fsdp_workers import ActorRolloutRefWorker

import asyncio
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import partial

def parallel_compute_score(evaluation_func, response_str, ground_truth, data_sources, max_workers=64):
    with tqdm(total=len(response_str)) as pbar:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(evaluation_func, data_sources[index], response_str[index], ground_truth[index]): index
                for index in range(len(response_str))
            }
            results = {}
            metadata = {}
            for future in as_completed(futures):
                index = futures[future]
                results[index] = future.result()
                pbar.update(1)

    return [results[i] for i in range(len(response_str))]


@hydra.main(config_path="config", config_name="generation", version_base=None)
def main(config):
    run_generation(config)


def run_generation(config) -> None:
    if not ray.is_initialized():
        # this is for local ray cluster
        ray.init(
            runtime_env={"env_vars": {"TOKENIZERS_PARALLELISM": "true", "NCCL_DEBUG": "WARN"}},
            num_cpus=config.ray_init.num_cpus,
        )

    ray.get(main_task.remote(config))


@ray.remote(num_cpus=1)
def main_task(config):
    pprint(OmegaConf.to_container(config, resolve=True))  # resolve=True will eval symbol values
    OmegaConf.resolve(config)

    # Check if output file already exists
    if os.path.exists(config.data.output_path):
        print(f"Output file {config.data.output_path} already exists. Skipping generation and proceeding to evaluation.")
        try:
            dataset = pd.read_parquet(config.data.output_path)
        except Exception as e:
            # Read json
            try:
                import json
                config.data.output_path = config.data.output_path.replace('.parquet', '.json')
                with open(config.data.output_path, 'r') as f:
                    dataset = pd.read_json(f)
            except Exception as e:
                # user polars
                import polars as pl
                config.data.output_path = config.data.output_path.replace('.json', '.parquet')
                dataset = pl.read_parquet(config.data.output_path)
    else:
        local_path = copy_to_local(config.model.path)
        trust_remote_code = config.data.get("trust_remote_code", False)
        tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)

        if config.rollout.temperature == 0.0:
            assert config.data.n_samples == 1, "When temperature=0, n_samples must be 1."
        assert config.data.n_samples >= 1, "n_samples should always >= 1"

        # read dataset. Note that the dataset should directly contain chat template format (e.g., a list of dictionary)
        try:
            dataset = pd.read_parquet(config.data.path)
            chat_lst = dataset[config.data.prompt_key].tolist()
            chat_lst = [chat.tolist() for chat in chat_lst]
        except Exception as e:
            # Read json
            import json
            config.data.path = config.data.path.replace('.parquet', '.json')
            with open(config.data.path, 'r') as f:
                dataset = pd.read_json(f)
            chat_lst = dataset[config.data.prompt_key].tolist()
            chat_lst = [chat for chat in chat_lst]

        print(f'dataset len: {len(dataset)}')

        # filter out too long prompts
        prompt_key = "prompt"
        dataset = dataset[dataset.apply(lambda doc: len(
            tokenizer.apply_chat_template(doc[prompt_key], add_generation_prompt=True)) <= config.rollout.prompt_length, axis=1)]

        print(f'filter dataset len: {len(dataset)}')

        tokenizer.padding_side = "left"
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        ray_cls_with_init = RayClassWithInitArgs(cls=ray.remote(ActorRolloutRefWorker), config=config, role="rollout")
        resource_pool = RayResourcePool(process_on_nodes=[config.trainer.n_gpus_per_node] * config.trainer.nnodes)
        wg = RayWorkerGroup(
            resource_pool=resource_pool,
            ray_cls_with_init=ray_cls_with_init,
            device_name=config.trainer.device,
        )
        wg.init_model()

        total_samples = len(dataset)
        config_batch_size = config.data.batch_size
        dp_size = wg.world_size // config.rollout.tensor_model_parallel_size
        num_batch = (total_samples // config_batch_size) + 1
        output_lst = []  # We'll reshape at the end

        for batch_idx in range(num_batch):
            print(f'[{batch_idx+1}/{num_batch}] Start to process.')
            batch_chat_lst = chat_lst[batch_idx * config_batch_size:(batch_idx + 1) * config_batch_size]

            # Repeat the batch n_samples times
            repeated_chat_lst = []
            for chat in batch_chat_lst:
                repeated_chat_lst.extend([chat] * config.data.n_samples)
            
            inputs = tokenizer.apply_chat_template(repeated_chat_lst,
                                                add_generation_prompt=True,
                                                padding=True,
                                                truncation=True,
                                                max_length=config.rollout.prompt_length,
                                                return_tensors='pt',
                                                return_dict=True,
                                                tokenize=True)

            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            position_ids = compute_position_id_with_mask(attention_mask)

            batch_dict = {'input_ids': input_ids, 'attention_mask': attention_mask, 'position_ids': position_ids}

            data = DataProto.from_dict(batch_dict)
            real_batch_size = data.batch['input_ids'].shape[0]

            if real_batch_size % dp_size != 0:
                dummy_data_size = dp_size - real_batch_size % dp_size
                dummy_data = data[:dummy_data_size]
                data = DataProto.concat([data, dummy_data])
                print(
                    f'dp_size {dp_size} is not divisible by real_batch_size {real_batch_size}, add {dummy_data_size} dummy data'
                )

            batch_size = data.batch['input_ids'].shape[0]
            assert batch_size % dp_size == 0, f'batch_size {batch_size} is not divisible by dp_size {dp_size}'

            print(f'[{batch_idx+1}/{num_batch}] Start to generate.')

            # Generate all samples at once
            print(len(data.batch['input_ids']))
            output = wg.generate_sequences(data)
            # Remove dummy data
            output = output[:real_batch_size]
            output_text = tokenizer.batch_decode(output.batch['input_ids'][:, -config.rollout.response_length:], skip_special_tokens=False)

            # Remove padding
            pad_token = tokenizer.pad_token
            output_text_unpad = []
            for text in output_text:
                output_text_unpad.append(text.replace(pad_token, ''))

            output_lst.extend(output_text_unpad)

        # Reshape output_lst from (total_samples,) to (n_data, n_samples)
        total_samples = len(output_lst)
        n_data = total_samples // config.data.n_samples
        output_lst = np.array(output_lst).reshape(n_data, config.data.n_samples).tolist()

        # Add to the data frame
        dataset['responses'] = output_lst

        # Write to a new parquet
        output_dir = os.path.dirname(config.data.output_path)
        makedirs(output_dir, exist_ok=True)
        dataset.to_parquet(config.data.output_path)

    if "livecodebench" in config.data.path:
        return

    # Compute evaluation metrics
    prompts = dataset[config.data.prompt_key]
    responses = dataset['responses']  # Using the generated responses
    data_sources = dataset[config.data.data_source_key]
    reward_model_data = dataset[config.data.reward_model_key]

    total = len(dataset)
    sequences_str_all, ground_truth_all, data_sources_all = [], [], []
    for i in range(total):
        response_lst = responses[i]
        data_source = data_sources[i]
        ground_truth = reward_model_data[i]['ground_truth']
        for r in response_lst:
            sequences_str_all.append(r)
            ground_truth_all.append(ground_truth)
            data_sources_all.append(data_source)

    assert len(sequences_str_all) == len(ground_truth_all) == len(data_sources_all)
    
    total_scores = []
    try:
        from rewards.general_reward import general_reward_fn
        total_scores = parallel_compute_score(
                general_reward_fn,
                sequences_str_all,
                ground_truth_all,
                data_sources_all,
            )
        assert len(total_scores) == len(sequences_str_all)
    except Exception as e:
        print(f"Unexpected error in batched reward computing. Setting all as 0.: {e}")
        total_scores = [0. for _ in range(len(sequences_str_all))]

    n_samples = config.data.n_samples
    total_scores = np.array(total_scores).reshape((-1, n_samples))
    pass_at_n = np.mean(np.max(total_scores, axis=-1))
    pass_at_1 = np.mean(total_scores)

    # Save metrics to CSV
    csv_path = config.data.output_path + '.pass.csv'
    dataset_name = os.path.basename(config.data.path)
    row_data = {
        'model_path': config.model.path,
        'dataset': dataset_name,
        'pass@1': pass_at_1,
        f'pass@{n_samples}': pass_at_n
    }
    file_exists = os.path.isfile(csv_path)
    with open(csv_path, mode='a', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=row_data.keys())
        if not file_exists:
            writer.writeheader()
        writer.writerow(row_data)

    # Convert the row data into a list of lists format for tabulate
    table_data = [[k, v] for k, v in row_data.items()]

    # Print table
    print(tabulate(table_data, headers=['Metric', 'Value'], tablefmt='grid'))

    # Convert boolean values to 0.0 or 1.0
    total_scores = [[1.0 if val else 0.0 for val in score_list] for score_list in total_scores]
    
    # Save the scores to results.json
    results_path = config.data.output_path + '.results.json'
    import json
    with open(results_path, 'w') as f:
        json.dump(total_scores, f)

    if config.trainer.get("use_wandb", True):
        wandb.init(project=config.trainer.project_name, name=config.trainer.experiment_name, id=config.trainer.experiment_name, resume="allow", allow_val_change=True)
        wandb.define_metric(f"val/{config.trainer.task_name}/pass@1", step_metric="global_step")
        wandb.define_metric(f"val/{config.trainer.task_name}/pass@{n_samples}", step_metric="global_step")
        wandb.log({f'val/{config.trainer.task_name}/pass@1': pass_at_1, f'val/{config.trainer.task_name}/pass@{n_samples}': pass_at_n, "global_step": config.trainer.global_step})
        wandb.finish(exit_code=0)


if __name__ == "__main__":
    main()
