diff --git a/scripts/train/run_archer2.0_qwen2.5_1.5b_code.sh b/scripts/train/run_archer2.0_qwen2.5_1.5b_code.sh
index edf6f21..fd8f3bf 100644
--- a/scripts/train/run_archer2.0_qwen2.5_1.5b_code.sh
+++ b/scripts/train/run_archer2.0_qwen2.5_1.5b_code.sh
@@ -35,11 +35,11 @@ train_prompt_mini_bsz=16
 MODEL_PATH=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
 CKPTS_DIR=./output/${project_name}/${exp_name} && mkdir -p $CKPTS_DIR
 data_dir=./data
-TRAIN_FILE=$data_dir/train/archer2.0-code-1.5b-train.json
+TRAIN_FILE=$data_dir/train/archer2.0-code-1.5b-train.parquet
 TEST_FILE=$data_dir/test/livecodebench_v5.json
 
 # Algorithm
-n_resp_per_prompt=16
+n_resp_per_prompt=8
 temperature=1.0
 top_p=1.0
 top_k=-1 # 0 for HF rollout, -1 for vLLM rollout
@@ -70,6 +70,21 @@ high_entropy_clip_ratio_high=0.4
 # Trainer
 use_overlong_filter=False
 
+# 如果本地没有 Archer2.0-Code-1.5B 的训练 parquet，就从 HF 下载并保存
+if [ ! -f "${TRAIN_FILE}" ]; then
+  python - << 'EOF'
+from datasets import load_dataset
+import os
+
+data_dir = "data/train"
+os.makedirs(data_dir, exist_ok=True)
+
+ds = load_dataset("Fate-Zero/Archer2.0-Code-1.5B", split="train")
+out_path = os.path.join(data_dir, "archer2.0-code-1.5b-train.parquet")
+ds.to_parquet(out_path)
+print("wrote", out_path)
+EOF
+fi
 
 python -m dapo.main_dapo \
     data.train_files="${TRAIN_FILE}" \
@@ -79,6 +94,7 @@ python -m dapo.main_dapo \
     data.truncation='error' \
     data.max_prompt_length=${max_prompt_length} \
     data.max_response_length=${max_response_length} \
+    +data.dataloader_num_workers=1 \
     data.gen_batch_size=${gen_prompt_bsz} \
     data.train_batch_size=${train_prompt_bsz} \
     actor_rollout_ref.rollout.n=${n_resp_per_prompt} \
@@ -144,7 +160,7 @@ python -m dapo.main_dapo \
     trainer.logger=['console','wandb'] \
     trainer.project_name="${project_name}" \
     trainer.experiment_name="${exp_name}" \
-    trainer.n_gpus_per_node=1 \
+    trainer.n_gpus_per_node=2 \
     trainer.nnodes="${nnodes}" \
     trainer.balance_batch=False \
     trainer.val_before_train=False \
diff --git a/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh b/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh
index 87643ee..a8bc280 100644
--- a/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh
+++ b/scripts/train/run_archer2.0_qwen2.5_1.5b_math.sh
@@ -32,14 +32,17 @@ gen_prompt_bsz=$((train_prompt_bsz * 1))
 train_prompt_mini_bsz=16
 
 # Paths
-MODEL_PATH=./models/DeepSeek-R1-Distill-Qwen-1.5B
+# 使用 Hugging Face 仓库名，而不是本地不存在的相对路径
+MODEL_PATH=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
 CKPTS_DIR=./output/${project_name}/${exp_name} && mkdir -p $CKPTS_DIR
 data_dir=./data
-TRAIN_FILE=$data_dir/train/archer2.0-math-1.5b-train.json
-TEST_FILE=$data_dir/test/aime2025.json
+train_dir=$data_dir/train
+test_dir=$data_dir/test
+TRAIN_FILE=$train_dir/archer2.0-math-1.5b-train.parquet
+TEST_FILE=$test_dir/archer2.0-math-1.5b-val.parquet
 
 # Algorithm
-n_resp_per_prompt=16
+n_resp_per_prompt=8
 temperature=1.0
 top_p=1.0
 top_k=-1 # 0 for HF rollout, -1 for vLLM rollout
@@ -70,6 +73,47 @@ high_entropy_clip_ratio_high=0.4
 # Trainer
 use_overlong_filter=False
 
+# 如果本地没有 Archer2.0-Math-1.5B 的 train/val parquet，就从 HF 下载并转换
+if [ ! -f "${TRAIN_FILE}" ] || [ ! -f "${TEST_FILE}" ]; then
+  python - << 'EOF'
+from datasets import load_dataset
+import pandas as pd
+import os, json
+
+train_dir = os.path.join("data", "train")
+test_dir = os.path.join("data", "test")
+os.makedirs(train_dir, exist_ok=True)
+os.makedirs(test_dir, exist_ok=True)
+
+ds = load_dataset("Fate-Zero/Archer2.0-Math-1.5B", split="train")
+split = ds.train_test_split(test_size=0.05, seed=42)
+train_ds = split["train"]
+val_ds = split["test"]
+
+def convert_and_save(hf_ds, out_path):
+    rows = []
+    for ex in hf_ds:
+        gt_list = ex.get("ground_truth") or []
+        gt = gt_list[0] if gt_list else ""
+        row = {
+            "prompt": [
+                {"role": "user", "content": ex["prompt"]}
+            ],
+            "reward_model": {"ground_truth": gt},
+            "data_source": ex.get("ability", "math"),
+        }
+        rows.append(row)
+
+    df = pd.DataFrame(rows)
+    df.to_parquet(out_path)
+    print("wrote", out_path, "rows:", len(df))
+
+train_out = os.path.join(train_dir, "archer2.0-math-1.5b-train.parquet")
+val_out = os.path.join(test_dir, "archer2.0-math-1.5b-val.parquet")
+convert_and_save(train_ds, train_out)
+convert_and_save(val_ds, val_out)
+EOF
+fi
 
 python -m dapo.main_dapo \
     data.train_files="${TRAIN_FILE}" \
@@ -79,6 +123,7 @@ python -m dapo.main_dapo \
     data.truncation='error' \
     data.max_prompt_length=${max_prompt_length} \
     data.max_response_length=${max_response_length} \
+    +data.dataloader_num_workers=1 \
     data.gen_batch_size=${gen_prompt_bsz} \
     data.train_batch_size=${train_prompt_bsz} \
     actor_rollout_ref.rollout.n=${n_resp_per_prompt} \
@@ -144,7 +189,7 @@ python -m dapo.main_dapo \
     trainer.logger=['console','wandb'] \
     trainer.project_name="${project_name}" \
     trainer.experiment_name="${exp_name}" \
-    trainer.n_gpus_per_node=8 \
+    trainer.n_gpus_per_node=2 \
     trainer.nnodes="${nnodes}" \
     trainer.balance_batch=False \
     trainer.val_before_train=False \
diff --git a/verl/trainer/ppo/core_algos.py b/verl/trainer/ppo/core_algos.py
index 8073bde..63a45da 100644
--- a/verl/trainer/ppo/core_algos.py
+++ b/verl/trainer/ppo/core_algos.py
@@ -554,8 +554,10 @@ def compute_policy_loss_archer(
 
     ratio = torch.exp(torch.clamp(log_prob - old_log_prob, min=-20.0, max=20.0))
 
-    negative_clip_ratio = torch.where(high_entropy_mask, torch.clamp(ratio, min=1-negative_low_entropy_clip_ratio_low, max=None), torch.clamp(ratio, min=1-negative_high_entropy_clip_ratio_low, max=None))
-    positive_clip_ratio = torch.where(high_entropy_mask, torch.clamp(ratio, min=None, max=1+positive_low_entropy_clip_ratio_high), torch.clamp(ratio, min=None, max=1+positive_high_entropy_clip_ratio_high))
+    # high_entropy_mask 可能是 LongTensor，这里统一转成 bool，避免 torch.where 类型错误
+    mask = high_entropy_mask.bool()
+    negative_clip_ratio = torch.where(mask, torch.clamp(ratio, min=1-negative_low_entropy_clip_ratio_low, max=None), torch.clamp(ratio, min=1-negative_high_entropy_clip_ratio_low, max=None))
+    positive_clip_ratio = torch.where(mask, torch.clamp(ratio, min=None, max=1+positive_low_entropy_clip_ratio_high), torch.clamp(ratio, min=None, max=1+positive_high_entropy_clip_ratio_high))
 
     clip_ratio = torch.where(advantages < 0, negative_clip_ratio, positive_clip_ratio)
 
