Using LocalLogger is deprecated. The constructor API will change
Checkpoint tracker file does not exist: /mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/./output/Archer2.0/Archer2.0-Qwen2.5-1.5B-Math/latest_checkpointed_iteration.txt
Training from scratch
Training Progress:   0%|          | 0/10500 [00:00<?, ?it/s]/mnt/iusers01/fatpou01/compsci01/h99859yz/miniconda3/envs/ASPO/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Batch Top-1 Frequency: 4
N-gram: Sequence_id 479:  \begin{aligned} & \sqrt{\alpha} = \frac{\sqrt{a + b}}{2}, \\ & \sqrt{\beta} = \frac{\sqrt{a - \sqrt{b}}}{2}, \\ & \sqrt{\gamma} = \frac{\sqrt{-\sqrt{b} +
Batch Top-2 Frequency: 3
N-gram: Sequence_id 8:  1 + t1 + t2 + t1 t2 - 2 = (1 - 2) + t1 + t2 + t1
Batch Top-3 Frequency: 3
N-gram: Sequence_id 8:  + t1 + t2 + t1 t2 - 2 = (1 - 2) + t1 + t2 + t1 t2
valid prompt: 42 12 10
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=1818480, ip=10.10.8.59, actor_id=d78ae6b1b2580dae0dc6cf8c01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x1479618fc460>)
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/single_controller/ray/base.py", line 645, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/single_controller/base/decorator.py", line 540, in inner
    return func(*args, **kwargs)
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/workers/fsdp_workers.py", line 601, in update_actor
    metrics = self.actor.update_policy(data=data)
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/utils/debug/performance.py", line 78, in f
    return self.log(decorated_function, *args, **kwargs)
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/utils/debug/performance.py", line 88, in log
    output = func(*args, **kwargs)
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/workers/actor/dp_actor.py", line 411, in update_policy
    pg_loss, pg_clipfrac_upper, pg_clipfrac_lower, negative_pg_clipfrac_dual, positive_pg_clipfrac_dual = compute_policy_loss_archer(
  File "/mnt/iusers01/fatpou01/compsci01/h99859yz/ASPO/Archer2.0/verl/trainer/ppo/core_algos.py", line 557, in compute_policy_loss_archer
    negative_clip_ratio = torch.where(high_entropy_mask, torch.clamp(ratio, min=1-negative_low_entropy_clip_ratio_low, max=None), torch.clamp(ratio, min=1-negative_high_entropy_clip_ratio_low, max=None))
RuntimeError: where expected condition to be a boolean tensor, but got a tensor with dtype Long
